% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM09: Dependência
Excessiva}\label{llm09-dependuxeancia-excessiva}

\subsubsection{Descrição}\label{descriuxe7uxe3o}

A Dependência Excessiva pode ocorrer quando um LLM produz informações
errôneas e as fornece de maneira autoritária. Embora os LLMs possam
produzir conteúdo criativo e informativo, também podem gerar conteúdo
factualmente incorreto, inapropriado ou inseguro. Isso é chamado de
alucinação ou confabulação. Quando pessoas ou sistemas confiam nessas
informações sem supervisão ou confirmação, pode resultar em violação de
segurança, desinformação, má comunicação, questões legais e danos à
reputação.

O código-fonte gerado pelo LLM pode introduzir vulnerabilidades de
segurança não percebidas. Isso representa um risco significativo para a
segurança operacional de aplicativos. Esses riscos destacam a
importância de processos rigorosos de revisão, com:

\begin{itemize}
\tightlist
\item
  Supervisão
\item
  Mecanismos contínuos de validação
\item
  Avisos sobre riscos
\end{itemize}

\subsubsection{Exemplos Comuns desta
Vulnerabilidade}\label{exemplos-comuns-desta-vulnerabilidade}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  LLM fornece informações imprecisas como resposta, declarando-as de
  maneira a sugerir alta autoridade. O sistema como um todo é projetado
  sem verificações e equilíbrios adequados para lidar com isso, e as
  informações enganam o usuário de uma maneira que leva a danos.
\item
  LLM sugere código inseguro ou com falhas, levando a vulnerabilidades
  quando incorporado a um sistema de software sem a devida supervisão ou
  verificação.
\end{enumerate}

\subsubsection{Estratégias de Prevenção e
Mitigação}\label{estratuxe9gias-de-prevenuxe7uxe3o-e-mitigauxe7uxe3o}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Monitore e revise regularmente as saídas do LLM. Use técnicas de auto
  consistência ou votação para filtrar texto inconsistente. Comparar
  múltiplas respostas do modelo para um único prompt pode ajudar a
  julgar melhor a qualidade e consistência da saída.
\item
  Verifique a saída do LLM com fontes externas confiáveis. Essa camada
  adicional de validação pode ajudar a garantir que as informações
  fornecidas pelo modelo sejam precisas e confiáveis.
\item
  Aprimore o modelo com ajuste fino ou embeddings para melhorar a
  qualidade da saída. Modelos pré-treinados genéricos têm mais chances
  de produzir informações imprecisas em comparação com modelos ajustados
  em um domínio específico. Técnicas como engenharia de prompts, ajuste
  eficiente de parâmetros (PEFT), ajuste total do modelo e prompts
  encadeados de pensamento podem ser empregadas para esse fim.
\item
  Implemente mecanismos automáticos de validação que possam verificar a
  saída gerada em relação a fatos ou dados conhecidos. Isso pode
  fornecer uma camada adicional de segurança e mitigar os riscos
  associados a alucinações.
\item
  Divida tarefas complexas em subtarefas gerenciáveis e atribua-as a
  diferentes agentes. Isso não apenas ajuda no gerenciamento da
  complexidade, mas também reduz as chances de alucinações, pois cada
  agente pode ser responsabilizado por uma tarefa menor.
\item
  Comunique claramente os riscos e limitações associados ao uso de LLMs.
  Isso inclui a possibilidade de imprecisões nas informações e outros
  riscos. Uma comunicação eficaz de riscos pode preparar os usuários
  para problemas potenciais e ajudá-los a tomar decisões informadas.
\item
  Construa APIs e interfaces de usuário que incentivem o uso responsável
  e seguro de LLMs. Isso pode envolver medidas como filtros de conteúdo,
  avisos ao usuário sobre imprecisões potenciais e rotulagem clara de
  conteúdo gerado por IA.
\item
  Ao usar LLMs em ambientes de desenvolvimento, estabeleça práticas e
  diretrizes seguras de codificação para evitar a integração de
  possíveis vulnerabilidades.
\end{enumerate}

\subsubsection{Exemplos de Cenários de
Ataque}\label{exemplos-de-cenuxe1rios-de-ataque}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Uma organização de notícias utiliza intensamente um LLM para gerar
  artigos. Um ator malicioso explora essa sobrecarga, alimentando
  informações enganosas ao LLM e causando a propagação de desinformação.
\item
  A IA inadvertidamente plagiariza um conteúdo, levando a problemas de
  direitos autorais e diminuindo a confiança na organização.
\item
  Uma equipe de desenvolvimento de software utiliza um sistema LLM para
  acelerar o processo de codificação. A sobrecarga nas sugestões do LLM
  introduz vulnerabilidades de segurança na aplicação devido a
  configurações padrão inseguras ou recomendações inconsistentes com
  práticas seguras de codificação.
\item
  Uma empresa de desenvolvimento de software usa um LLM para auxiliar os
  desenvolvedores. O LLM sugere uma biblioteca ou pacote de código
  inexistente, e um desenvolvedor, confiando na IA, integra
  inadvertidamente um pacote malicioso no software da empresa. Isso
  destaca a importância de verificar as sugestões do LLM, especialmente
  ao envolver código ou bibliotecas de terceiros. \#\#\# Links de
  Referência
\item
  \href{https://towardsdatascience.com/llm-hallucinations-ec831dcd7786}{Understanding
  LLM Hallucinations}: \textbf{Towards Data Science}
\item
  \href{https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/}{How
  Should Companies Communicate the Risks of Large Language Models to
  Users?}: \textbf{Techpolicy}
\item
  \href{https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/}{A
  news site used AI to write articles. It was a journalistic disaster}:
  \textbf{Washington Post}
\item
  \href{https://vulcan.io/blog/ai-hallucinations-package-risk}{AI
  Hallucinations: Package Risk}: \textbf{Vulcan.io}
\item
  \href{https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/}{How
  to Reduce the Hallucinations from Large Language Models}: \textbf{The
  New Stack}
\item
  \href{https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination}{Practical
  Steps to Reduce Hallucination}: \textbf{Victor Debia}
\end{enumerate}

\end{document}
