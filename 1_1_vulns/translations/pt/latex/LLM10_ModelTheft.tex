% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM10: Model Theft}\label{llm10-model-theft}

\subsubsection{Descrição}\label{descriuxe7uxe3o}

Esta entrada refere-se ao acesso não autorizado e à exfiltração de
modelos LLM por atores maliciosos ou APTs. Isso ocorre quando modelos
LLM proprietários (sendo propriedade intelectual valiosa) são
comprometidos, fisicamente roubados, copiados ou quando pesos e
parâmetros são extraídos para criar um equivalente funcional. O impacto
do roubo de modelos LLM pode incluir perda econômica e de reputação da
marca, erosão da vantagem competitiva, uso não autorizado do modelo ou
acesso não autorizado a informações sensíveis contidas no modelo.

O roubo de LLMs representa uma preocupação significativa de segurança à
medida que os modelos de linguagem se tornam cada vez mais poderosos e
prevalentes. Organizações e pesquisadores devem priorizar medidas de
segurança robustas para proteger seus modelos LLM, garantindo a
confidencialidade e integridade de sua propriedade intelectual. A adoção
de um framework de segurança abrangente que inclua controles de acesso,
criptografia e monitoramento contínuo é crucial para mitigar os riscos
associados ao roubo de modelos LLM e proteger os interesses de
indivíduos e organizações que dependem de LLMs.

\subsubsection{Exemplos Comuns desta
Vulnerabilidade}\label{exemplos-comuns-desta-vulnerabilidade}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Um atacante explora uma vulnerabilidade na infraestrutura de uma
  empresa para obter acesso não autorizado ao repositório de modelos LLM
  por meio de configurações inadequadas na rede ou nas configurações de
  segurança da aplicação.
\item
  Um cenário de ameaça interna em que um funcionário descontente vaza
  modelos ou artefatos relacionados.
\item
  Um atacante consulta a API do modelo usando inputs cuidadosamente
  elaborados e técnicas de injeção de prompt para coletar um número
  suficiente de saídas para criar um modelo sombra.
\item
  Um atacante malicioso consegue contornar técnicas de filtragem de
  entrada do LLM para realizar um ataque de canal lateral e, finalmente,
  extrair pesos do modelo e informações de arquitetura para um recurso
  controlado remotamente.
\item
  O vetor de ataque para extração de modelo envolve consultar o LLM com
  um grande número de prompts sobre um tópico específico. As saídas do
  LLM podem ser usadas para ajustar outro modelo. No entanto, alguns
  pontos importantes sobre esse ataque são:

  \begin{itemize}
  \tightlist
  \item
    O atacante deve gerar um grande número de prompts direcionados. Se
    os prompts não forem específicos o suficiente, as saídas do LLM
    serão inúteis.
  \item
    As saídas dos LLMs podem às vezes conter respostas alucinadas, o que
    significa que o atacante pode não ser capaz de extrair o modelo
    inteiro, já que algumas saídas podem ser sem sentido.

    \begin{itemize}
    \tightlist
    \item
      Não é possível replicar um LLM 100\% por meio da extração de
      modelo. No entanto, o atacante será capaz de replicar parcialmente
      um modelo.
    \end{itemize}
  \end{itemize}
\item
  O vetor de ataque para \textbf{\emph{replicação funcional do modelo}}
  envolve o uso do modelo-alvo por meio de prompts para gerar dados
  sintéticos de treinamento (uma abordagem chamada ``autoinstrução'')
  para então usá-lo e ajustar outro modelo fundamental para produzir um
  equivalente funcional. Isso contorna as limitações da extração baseada
  em consultas tradicional usada no Exemplo 5 e foi usado com sucesso em
  pesquisas sobre o uso de um LLM para treinar outro LLM. Embora, no
  contexto dessa pesquisa, a replicação do modelo não seja um ataque. A
  abordagem poderia ser usada por um atacante para replicar um modelo
  proprietário com uma API pública.
\end{enumerate}

O uso de um modelo roubado, como modelo sombra, pode ser usado para
realizar ataques adversários, incluindo acesso não autorizado a
informações sensíveis contidas no modelo ou experimentar com inputs
adversários para realizar injeções avançadas de prompt.

\subsubsection{Estratégias de Prevenção e
Mitigação}\label{estratuxe9gias-de-prevenuxe7uxe3o-e-mitigauxe7uxe3o}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implementar controles de acesso robustos (por exemplo, RBAC e o
  princípio do menor privilégio) e mecanismos de autenticação sólidos
  para limitar o acesso não autorizado a repositórios de modelos LLM e
  ambientes de treinamento.

  \begin{itemize}
  \tightlist
  \item
    Isso é especialmente válido para os três primeiros exemplos comuns,
    que podem causar essa vulnerabilidade devido a ameaças internas,
    configuração inadequada e/ou controles de segurança fracos sobre a
    infraestrutura que abriga modelos LLM, pesos e arquitetura nos quais
    um ator malicioso poderia infiltrar-se de dentro ou fora do
    ambiente.
  \item
    O rastreamento, verificação e vulnerabilidades de dependência de
    gerenciamento de fornecedores são tópicos importantes para prevenir
    exploração de ataques à cadeia de suprimentos.
  \end{itemize}
\item
  Restringir o acesso do LLM a recursos de rede, serviços internos e
  APIs.

  \begin{itemize}
  \tightlist
  \item
    Isso é especialmente válido para todos os exemplos comuns, pois
    cobre riscos e ameaças internas, mas também controla o que a
    aplicação LLM ``\emph{tem acesso}'' e, assim, pode ser um mecanismo
    ou etapa de prevenção para evitar ataques de canal lateral.
  \end{itemize}
\item
  Usar um Inventário ou Registro Centralizado de Modelos de ML para
  modelos usados em produção. Ter um registro centralizado de modelos
  impede o acesso não autorizado a modelos de ML por meio de controles
  de acesso, autenticação e capacidade de monitoramento/registro, que
  são bases sólidas para a governança. Ter um repositório centralizado
  também é benéfico para coletar dados sobre algoritmos usados pelos
  modelos para fins de conformidade, avaliações de risco e mitigação de
  riscos.
\item
  Monitorar regularmente e auditar logs de acesso e atividades
  relacionadas a repositórios de modelos LLM para detectar e responder
  prontamente a qualquer comportamento suspeito ou não autorizado.
\item
  Automatizar implementações MLOps com fluxos de trabalho de governança,
  rastreamento e aprovação para reforçar controles de acesso e
  implementação dentro da infraestrutura.
\item
  Implementar controles e estratégias de mitigação para reduzir o risco
  de técnicas de injeção de prompt causarem ataques de canal lateral.
\item
  Limitar a taxa de chamadas de API quando aplicável e/ou implementar
  filtros para reduzir o risco de exfiltração de dados das aplicações
  LLM, ou implementar técnicas para detectar (por exemplo, DLP)
  atividade de extração de outros sistemas de monitoramento.
\item
  Implementar treinamento de robustez adversária para ajudar a detectar
  consultas de extração e reforçar medidas de segurança física.
\item
  Implementar um framework de marca d'água nas etapas de incorporação e
  detecção do ciclo de vida de um LLM.
\end{enumerate}

\subsubsection{Exemplos de Cenários de
Ataque}\label{exemplos-de-cenuxe1rios-de-ataque}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Um atacante explora uma vulnerabilidade na infraestrutura de uma
  empresa para obter acesso não autorizado ao repositório de modelos
  LLM. O atacante procede a exfiltrar modelos LLM valiosos e os usa para
  lançar um serviço de processamento de linguagem concorrente ou extrair
  informações sensíveis, causando danos financeiros significativos à
  empresa original.
\item
  Um funcionário descontente vaza modelos ou artefatos relacionados. A
  exposição pública desse cenário aumenta o conhecimento dos atacantes
  para ataques adversários de caixa cinza ou, alternativamente, rouba
  diretamente a propriedade disponível.
\item
  Um atacante consulta a API com inputs cuidadosamente selecionados e
  coleta um número suficiente de saídas para criar um modelo sombra.
\item
  Uma falha de controle de segurança está presente na cadeia de
  suprimentos e leva a vazamentos de dados de informações proprietárias
  do modelo.
\item
  Um atacante malicioso contorna as técnicas de filtragem de entrada e
  preâmbulos do LLM para realizar um ataque de canal lateral e recuperar
  informações do modelo para um recurso controlado remotamente sob seu
  controle.
\end{enumerate}

\subsubsection{Links de Referência}\label{links-de-referuxeancia}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse}{Meta's
  powerful AI language model has leaked online}: \textbf{The Verge}
\item
  \href{https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/}{Runaway
  LLaMA \textbar{} How Meta's LLaMA NLP model leaked}: \textbf{Deep
  Learning Blog}
\item
  \href{https://atlas.mitre.org/tactics/AML.TA0000}{AML.TA0000 ML Model
  Access}: \textbf{MITRE ATLAS}
\item
  \href{https://arxiv.org/pdf/1803.05847.pdf}{I Know What You See:}:
  \textbf{Arxiv White Paper}
\item
  \href{https://www.computer.org/csdl/proceedings-article/sp/2023/933600a432/1He7YbsiH4c}{D-DAE:
  Defense-Penetrating Model Extraction Attacks:}: \textbf{Computer.org}
\item
  \href{https://ieeexplore.ieee.org/document/10080996}{A Comprehensive
  Defense Framework Against Model Extraction Attacks}: \textbf{IEEE}
\item
  \href{https://crfm.stanford.edu/2023/03/13/alpaca.html}{Alpaca: A
  Strong, Replicable Instruction-Following Model}: \textbf{Stanford
  Center on Research for Foundation Models (CRFM)}
\item
  \href{https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html}{How
  Watermarking Can Help Mitigate The Potential Risks Of LLMs?}:
  \textbf{KD Nuggets}
\end{enumerate}

\end{document}
