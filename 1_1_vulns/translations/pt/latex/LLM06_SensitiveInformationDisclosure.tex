% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM06: Divulgação de Informações
Sensíveis}\label{llm06-divulgauxe7uxe3o-de-informauxe7uxf5es-sensuxedveis}

\subsubsection{Descrição}\label{descriuxe7uxe3o}

As aplicações LLM têm o potencial de revelar informações sensíveis,
algoritmos proprietários ou outros detalhes confidenciais por meio de
suas saídas. Isso pode resultar em acesso não autorizado a dados
sensíveis, violações de propriedade intelectual, violações de
privacidade e outras falhas de segurança. É importante que os
consumidores de aplicações LLM estejam cientes de como interagir com
segurança com os LLMs e identificar os riscos associados à entrada não
intencional de dados sensíveis que podem ser posteriormente retornados
pelo LLM em outras saídas.

Para mitigar esse risco, as aplicações LLM devem realizar uma
sanitização adequada dos dados para evitar que os dados do usuário
entrem nos dados de treinamento do modelo. Os proprietários de
aplicações LLM também devem ter políticas apropriadas de Termos de Uso
disponíveis para conscientizar os consumidores sobre como seus dados são
processados e sobre a capacidade de optar por não incluir seus dados no
modelo de treinamento.

A interação consumidor-aplicação LLM forma uma fronteira de confiança
bidirecional, onde não podemos confiar inerentemente na entrada
cliente-\textgreater LLM ou na saída LLM-\textgreater cliente. É
importante observar que esta vulnerabilidade assume que certos
pré-requisitos estão fora do escopo, como exercícios de modelagem de
ameaças, segurança de infraestrutura e sandboxing adequado. Adicionar
restrições dentro da solicitação do sistema em torno dos tipos de dados
que o LLM deve retornar pode fornecer alguma mitigação contra a
divulgação de informações sensíveis, mas a natureza imprevisível dos
LLMs significa que tais restrições nem sempre serão respeitadas e podem
ser contornadas por meio de injeção de prompt ou outros vetores.

\subsubsection{Exemplos Comuns de
Vulnerabilidade}\label{exemplos-comuns-de-vulnerabilidade}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Filtragem incompleta ou inadequada de informações sensíveis nas
  respostas do LLM.
\item
  Memorização ou sobreajuste (overfitting) de dados sensíveis no
  processo de treinamento do LLM.
\item
  Divulgação não intencional de informações confidenciais devido à
  interpretação inadequada do LLM, falta de métodos de limpeza de dados
  ou erros.
\end{enumerate}

\subsubsection{Estratégias de Prevenção e
Mitigação}\label{estratuxe9gias-de-prevenuxe7uxe3o-e-mitigauxe7uxe3o}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Integre técnicas adequadas de sanitização e limpeza de dados para
  evitar que os dados do usuário entrem nos dados de treinamento do
  modelo.
\item
  Implemente métodos robustos de validação e sanitização de entrada para
  identificar e filtrar inputs potencialmente maliciosos e prevenir que
  o modelo seja envenenado.
\item
  Ao enriquecer o modelo com dados e se
  \href{https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/wiki/Definitions}{ajustar
  finamente} um modelo (ou seja, alimentar o modelo antes ou durante a
  implantação):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Qualquer informação considerada sensível nos dados de ajuste fino
    tem o potencial de ser revelada a um usuário. Portanto, aplique a
    regra do menor privilégio e não treine o modelo em informações que o
    usuário de mais alto privilégio pode acessar e que podem ser
    exibidas a um usuário de menor privilégio.
  \item
    O acesso a fontes de dados externas (orquestação de dados em tempo
    de execução) deve ser limitado.
  \item
    Aplique métodos rígidos de controle de acesso a fontes de dados
    externas e uma abordagem rigorosa para manter uma cadeia de
    suprimentos segura.
  \end{enumerate}
\end{enumerate}

\subsubsection{Cenários de Ataque
Exemplo}\label{cenuxe1rios-de-ataque-exemplo}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  O usuário legítimo A, sem suspeitas, é exposto a certos dados de
  outros usuários por meio do LLM ao interagir com a aplicação LLM de
  maneira não maliciosa.
\item
  O usuário A direciona um conjunto bem elaborado de prompts para burlar
  filtros de entrada e sanitização do LLM, fazendo com que ele revele
  informações sensíveis (PII) sobre outros usuários da aplicação.
\item
  Dados pessoais, como PII, vazam para o modelo por meio de dados de
  treinamento, devido à negligência do próprio usuário ou da aplicação
  LLM. Esse cenário poderia aumentar o risco e a probabilidade dos
  cenários 1 ou 2 acima.
\end{enumerate}

\subsubsection{Links de Referência}\label{links-de-referuxeancia}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt}{AI
  data leak crisis: New tool prevents company secrets from being fed to
  ChatGPT}: \textbf{Fox Business}
\item
  \href{https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/}{Lessons
  learned from ChatGPT's Samsung leak}: \textbf{Cybernews}
\item
  \href{https://cohere.com/terms-of-use}{Cohere - Terms Of Use}
  \textbf{Cohere}
\item
  \href{https://aivillage.org/large\%20language\%20models/threat-modeling-llm/}{A
  threat modeling example}: \textbf{AI Village}
\item
  \href{https://owasp.org/www-project-ai-security-and-privacy-guide/}{OWASP
  AI Security and Privacy Guide}: \textbf{OWASP AI Security \& Privacy
  Guide}
\item
  \href{https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html}{Ensuring
  the Security of Large Language Models}: \textbf{Experts Exchange}
\end{enumerate}

\end{document}
