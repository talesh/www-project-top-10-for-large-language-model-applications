% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM06: Divulgazione di Informazioni
Sensibili}\label{llm06-divulgazione-di-informazioni-sensibili}

\subsubsection{Descrizione}\label{descrizione}

Le applicazioni LLM possono rivelare informazioni sensibili, algoritmi
proprietari o altri dettagli confidenziali attraverso i loro output. Ciò
può portare ad accessi non autorizzati, esposizione di dati sensibili,
proprietà intellettuale, violazioni della privacy e altre problematiche
di sicurezza. È importante che gli utenti delle applicazioni LLM siano
consapevoli di come interagire in modo sicuro con gli LLM e identificare
i rischi associati all'inserimento involontario di dati sensibili i
quali potrebbero poi essere divulgati dal LLM in altri contesti.

Per mitigare questo rischio, le applicazioni LLM dovrebbero implementare
un'adeguata sanificazione dei dati per impedire che i dati degli utenti
entrino indiscriminatamente nei set di dati di addestramento del
modello. I gestori di applicazioni LLM dovrebbero inoltre fornire dei
Termini di Utilizzo adeguati, facilmente accessibili per informare gli
utenti come vengono gestiti i loro dati, e l'opzione ben visibile per
negare il consenso a includere i loro dati nei processi di addestramento
del modello.

L'interazione tra l'utente e l'applicazione LLM instaura un contesto di
fiducia reciproca, nel quale non possiamo fidarci intrinsecamente né
dell'input utente-\textgreater LLM né dell'output
LLM-\textgreater utente. È importante notare che questa vulnerabilità
presuppone che certi prerequisiti siano assicurati al di fuori del
presente ambito di analisi, fra questi gli esercizi di modellazione
delle minacce (threat modeling), la protezione delle infrastrutture e
una segregazione adeguata degli ambienti di esecuzione. Aggiungere
restrizioni all'interno del prompt del sistema riguardo ai tipi di dati
che il LLM dovrebbe restituire può fornire una mitigazione parziale
contro la divulgazione di informazioni sensibili, tuttavia, data
l'imprevedibilità degli LLM è possibile che tali restrizioni potrebbero
non essere sempre efficaci e potrebbero essere aggirate tramite
iniezione di prompt o altri metodi.

\subsubsection{Esempi comuni di
vulnerabilità}\label{esempi-comuni-di-vulnerabilituxe0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Filtraggio incompleto o inefficace delle informazioni sensibili
  presenti nelle risposte del LLM.
\item
  Sovradattamento o memorizzazione di dati sensibili nel processo di
  addestramento del LLM.
\item
  Divulgazione involontaria di informazioni confidenziali a causa di
  errata interpretazione da parte del LLM, mancanza di metodi di pulizia
  dei dati o errori.
\end{enumerate}

\subsubsection{Strategie di prevenzione e
mitigazione}\label{strategie-di-prevenzione-e-mitigazione}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Integrare adeguate tecniche di sanificazione e pulizia dei dati per
  impedire che i dati degli utenti entrino nei set di dati di
  addestramento del modello.
\item
  Implementare metodi robusti di validazione e sanificazione degli input
  per identificare ed escludere potenziali input malevoli per prevenire
  l'avvelenamento (poisoning) del modello.
\item
  Quando si arricchisce il modello con dati e se si effettua il
  ``fine-tuning'' (rif.7) di un modello (ad esempio, dati inseriti nel
  modello prima o durante il rilascio):
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Qualunque informazione sensibile nei dati di fine-tuning ha il
  potenziale di essere rivelato a un utente. Pertanto, si raccomanda di
  applicare la buona pratica di minimizzazione dei privilegi di accesso,
  e di non addestrare il modello su informazioni a cui un utente con
  privilegi elevati può accedere poiché potrebbero inavvertitamente
  essere mostrate a un utente con privilegi inferiori.
\item
  L'accesso a fonti di dati esterne (orchestrazione dei dati in tempo
  reale) dovrebbe essere limitato.
\item
  Applicare metodi stringenti di controllo degli accessi alle fonti di
  dati esterne e un approccio rigoroso nella gestione di una catena di
  approvvigionamento sicura.
\end{itemize}

\subsubsection{Esempi di scenari di
attacco}\label{esempi-di-scenari-di-attacco}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  L'utente legittimo e ignaro A viene esposto a dati di altri utenti
  tramite il LLM quando interagisce con l'applicazione LLM in modo non
  malevolo.
\item
  L'utente A indirizza un insieme ben congegnato di prompt per bypassare
  i filtri di input e la sanificazione del LLM per far sì che divulghi
  informazioni sensibili (PII) su altri utenti dell'applicazione.
\item
  Dati personali (PII) vengono introdotti nel modello durante il
  processo di addestramento a causa di negligenza da parte dell'utente
  stesso o dell'applicazione LLM. Questo scenario potrebbe aumentare il
  rischio e la probabilità degli scenari 1 o 2 sopra descritti.
\end{enumerate}

\subsubsection{Riferimenti e link
(Inglese)}\label{riferimenti-e-link-inglese}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt}{AI
  data leak crisis: New tool prevents company secrets from being fed to
  ChatGPT}: \textbf{Fox Business}
\item
  \href{https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/}{Lessons
  learned from ChatGPT's Samsung leak}: \textbf{Cybernews}
\item
  \href{https://cohere.com/terms-of-use}{Cohere - Terms Of Use}:
  \textbf{Cohere}
\item
  \href{https://aivillage.org/large\%20language\%20models/threat-modeling-llm/}{A
  threat modeling example}: \textbf{AI Village}
\item
  \href{https://owasp.org/www-project-ai-security-and-privacy-guide/}{OWASP
  AI Security and Privacy Guide}: \textbf{OWASP AI Security \& Privacy
  Guide}
\item
  \href{https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html}{Ensuring
  the Security of Large Language Models}: \textbf{Experts Exchange}
\item
  \href{https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/wiki/Definitions}{fine-tuning}
\end{enumerate}

\end{document}
