% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM09: Eccessivo
Affidamento}\label{llm09-eccessivo-affidamento}

\subsubsection{Descrizione}\label{descrizione}

L'Eccessivo Affidamento si manifesta quando un LLM produce risultati
erronei, presentati con una falsa aura di autorità. Mentre gli LLM
usualmente producono materiale creativo e informativo, questi possono
anche generare contenuti fattualmente scorretti, inappropriati o
pericolosi. Questo fenomeno è noto sotto il nome di allucinazione o
confabulazione. Quando persone o sistemi si affidano incondizionatamente
a queste informazioni, si possono generare violazioni della sicurezza,
disinformazione, comunicazione errata, problemi legali, e danni
reputazionali.

Il codice Sorgente generato da LLM, può introdurre silenziosamente delle
vulnerabilità di sicurezza. Ciò espone a rischi significativi per
l'integrità operativa, e la sicurezza delle applicazioni. Questi rischi
sottolineano l'importanza di continui processi di verifica, attraverso:
* Supervisione * Meccanismi di validazione continua * Note agli
utilizzatori sui rischi connessi all'utilizzo di tali tecnologie.

\subsubsection{Esempi comuni di
vulnerabilità}\label{esempi-comuni-di-vulnerabilituxe0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Un LLM fornisce informazioni inesatte come risposta, presentate in
  maniera molto autorevole. L'intero sistema è progettato senza adeguati
  controlli e bilanciamento per gestire queste situazioni e le
  informazioni generate possono traviare l'utente in maniera da generare
  potenziali danni.
\item
  Un LLM propone codice insicuro o difettoso, introducendo vulnerabilità
  quando incorporato in un sistema software senza controlli preventivi o
  verifiche puntuali su di esso.
\end{enumerate}

\subsubsection{Strategie di prevenzione e
mitigazione}\label{strategie-di-prevenzione-e-mitigazione}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Monitoraggio costante e revisione degli output del LLM. Utilizzo di
  tecniche rivolte all'auto-consistenza o metodi di voto per filtrare
  testi incoerenti. Analizzare e confrontare le varie risposte fornite
  dal modello a un unico prompt può aiutare a valutare meglio la qualità
  e la coerenza degli output.
\item
  Controlli incrociati sul modello rispetto a fonti esterne certificate.
  Questo livello supplementare di validazione può aiutare ad assicurare
  che le informazioni prodotte dal modello siano accurate e affidabili.
\item
  Migliorare il modello con metodi di taratura fine (fine-tuning) o
  incorporazione (embedding) per migliorare la qualità degli output.
  Modelli generici pre-addestrati possono generare informazioni meno
  accurate con maggiore probabilità rispetto a modelli addestrati su un
  dominio specifico. Tecniche come l'ingegnerizzazione dei prompt,
  ottimizzazione efficiente dei parametri (PET), ottimizzazione
  dell'intero modello, e prompting basato su catena di pensiero (chain
  of thoughts prompting) possono ottimizzarne le prestazioni.
\item
  Implementazione di meccanismi di validazione automatica che possono
  effettuare controlli comparativi sugli output generati, rispetto a
  fatti o dati consolidati. Questo può fornire un livello di sicurezza
  aggiuntiva e mitigare i rischi associati alle allucinazioni.
\item
  Suddivisione di compiti complessi in singole azioni parziali più
  gestibili, e assegnazione ad agenti specializzati. Questo non solo
  aiuta a gestire la complessità, ma anche a ridurre le probabilità di
  allucinazione essendo ogni agente responsabile di obiettivi più
  limitati e controllabili.
\item
  Comunicare in modo trasparente i rischi e le limitazioni associate con
  l'utilizzo degli LLM. Ciò include potenziale inesattezza delle
  informazioni, e altri rischi connessi. Una chiara comunicazione dei
  rischi prepara gli utenti a potenziali problemi e li aiuta a prendere
  decisioni in modo consapevole.
\item
  Costruire API e interfacce utente che incoraggino un utilizzo
  responsabile degli LLM. Questo include misure come filtri di
  contenuti, avvisi di potenziale inesattezza e chiara indicazione dei
  materiali generati dall'AI.
\item
  Quando si utilizzano LLM in ambienti di sviluppo, è necessario
  stabilire criteri di codifica sicura e linee guida che prevengano
  l'introduzione di potenziali vulnerabilità.
\end{enumerate}

\subsubsection{Esempi di scenari di
attacco}\label{esempi-di-scenari-di-attacco}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Una testata giornalistica si affida eccessivamente a un LLM per
  generare i propri articoli e notizie. Un malintenzionato può sfruttare
  questa eccessiva dipendenza, iniettando nel LLM informazioni
  fuorvianti, che favoriscono la diffusione di notizie false o mirate.
\item
  L'intelligenza artificiale plagia involontariamente contenuti,
  causando problemi di proprietà intellettuale che minano la credibilità
  dell'organizzazione coinvolta.
\item
  Un team di sviluppo software utilizza un LLM per accelerare la
  scrittura di codice. Un'eccessiva dipendenza dai suggerimenti dell'AI
  introduce vulnerabilità nelle applicazioni generate a causa di
  parametri di default insicuro o raccomandazioni che non rispettano le
  pratiche di sviluppo sicuro.
\item
  Un'azienda che si occupa di sviluppo software utilizza un LLM per
  fornire assistenza agli sviluppatori. Il modello suggerisce una
  libreria o un pacchetto inesistenti e uno sviluppatore, affidandosi
  all'AI, integra senza saperlo un pacchetto malevolo nel software
  venduto dall'azienda. Questo esempio sottolinea l'importanza dei
  controlli incrociati sui suggerimenti del modello, specialmente in
  relazione a codice o librerie di terze parti.
\end{enumerate}

\subsubsection{Riferimenti e link
(Inglese)}\label{riferimenti-e-link-inglese}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://towardsdatascience.com/llm-hallucinations-ec831dcd7786}{Understanding
  LLM Hallucinations}: \textbf{Towards Data Science}
\item
  \href{https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/}{How
  Should Companies Communicate the Risks of Large Language Models to
  Users?}: \textbf{Techpolicy}
\item
  \href{https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/}{A
  news site used AI to write articles. It was a journalistic disaster}:
  \textbf{Washington Post}
\item
  \href{https://vulcan.io/blog/ai-hallucinations-package-risk}{AI
  Hallucinations: Package Risk}: \textbf{Vulcan.io}
\item
  \href{https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/}{How
  to Reduce the Hallucinations from Large Language Models}: \textbf{The
  New Stack}
\item
  \href{https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination}{Practical
  Steps to Reduce Hallucination}: \textbf{Victor Debia}
\end{enumerate}

\end{document}
