% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM09: Übermäßige
Abhängigkeit}\label{llm09-uxfcbermuxe4uxdfige-abhuxe4ngigkeit}

\subsubsection{Beschreibung}\label{beschreibung}

Übermäßige Abhängigkeit kann entstehen, wenn ein LLM fehlerhafte
Informationen produziert und diese als authentisch darstellt. Während
LLMs kreative und informative Inhalte produzieren können, können sie
auch Inhalte produzieren, die faktisch falsch, unangemessen oder
unsicher sind. Dies wird als Halluzination oder Konfabulation
bezeichnet. Wenn Menschen oder Systeme diesen Informationen ohne
Überprüfung oder Bestätigung vertrauen, kann dies zu
Sicherheitsverletzungen, Fehlinformationen, falscher Kommunikation,
rechtlichen Problemen und Rufschädigung führen.

Von LLM erzeugter Quellcode kann unbemerkt Sicherheitslücken einführen.
Dies stellt ein erhebliches Risiko für die Betriebs- und
Anwendungssicherheit dar. Diese Risiken unterstreichen die Bedeutung
strenger Verifikationsprozesse:

\begin{itemize}
\tightlist
\item
  Überprüfung
\item
  kontinuierliche Validierungsmechanismen
\item
  Haftungsausschlüsse für Risiken
\end{itemize}

\subsubsection{Gängige Beispiele für
Schwachstellen}\label{guxe4ngige-beispiele-fuxfcr-schwachstellen}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ein LLM antwortet mit ungenauen Informationen auf eine Art und Weise,
  die es höchst vertrauenswürdig wirken lässt. Das Gesamtsystem ist ohne
  angemessene Kontrollen und Überprüfungen konzipiert, und die
  Informationen führen Personen in einer Weise in die Irre, die zu
  Schäden führen kann.
\item
  Das LLM schlägt unsicheren oder fehlerhaften Code vor, der zu
  Schwachstellen führt, wenn er ohne angemessene Aufsicht oder
  Überprüfung in ein Softwaresystem eingebaut wird.
\end{enumerate}

\subsubsection{Präventions- und
Mitigationsstrategien}\label{pruxe4ventions--und-mitigationsstrategien}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Überwachen und überprüfen Sie regelmäßig die Ergebnisse des LLMs.
  Verwenden Sie Selbstkonsistenz- oder Voting-Techniken, um
  inkonsistenten Text herauszufiltern. Der Vergleich mehrerer
  Modellantworten auf eine Anfrage kann helfen, die Qualität und
  Konsistenz der Ausgabe zu beurteilen.
\item
  Überprüfen Sie die LLM-Ausgabe mit vertrauenswürdigen externen
  Quellen. Diese zusätzliche Ebene der Validierung kann dazu beitragen,
  dass die vom Modell gelieferten Informationen genau und zuverlässig
  sind.
\item
  Verbessern Sie das Modell durch Fine-Tuning oder Embeddings, um die
  Ausgabequalität zu verbessern. Allgemeine, vortrainierte Modelle
  liefern mit größerer Wahrscheinlichkeit ungenaue Informationen als
  Modelle, die in einem bestimmten Bereich angepasst wurden. Techniken
  wie Prompt Engineering, parameter efficient tuning (PET),
  vollständiges Model-Tuning und Chain-of-Thought-Prompting können zu
  diesem Zweck eingesetzt werden.
\item
  Implementieren Sie automatische Validierungsmechanismen, die die
  generierte Ausgabe mit bekannten Fakten oder Daten vergleichen können.
  Dies kann eine zusätzliche Sicherheitsebene bieten und das Risiko von
  Halluzinationen verringern.
\item
  Zerlegen Sie komplexe Aufgaben in handhabbare Unteraufgaben und weisen
  Sie sie verschiedenen Agenten zu. Dies hilft nicht nur bei der
  Bewältigung der Komplexität, sondern verringert auch die
  Wahrscheinlichkeit von Halluzinationen, da jeder Agent für eine
  kleinere Aufgabe verantwortlich gemacht werden kann.
\item
  Kommunizieren Sie klar die Risiken und Einschränkungen, die mit der
  Verwendung von LLMs verbunden sind. Dies schließt das Potenzial für
  Informationsungenauigkeiten und andere Risiken ein. Eine effektive
  Risikokommunikation kann die Nutzenden auf mögliche Probleme
  vorbereiten und ihnen helfen, informierte Entscheidungen zu treffen.
\item
  Entwickeln Sie APIs und Benutzeroberflächen, die eine
  verantwortungsvolle und sichere Nutzung von LLMs fördern. Dies kann
  Maßnahmen wie Inhaltsfilter, Benutzerwarnungen vor potenziellen
  Ungenauigkeiten und eine klare Kennzeichnung von KI-generierten
  Inhalten umfassen.
\item
  Bei der Verwendung von LLMs in Entwicklungsumgebungen sollten sichere
  Programmierpraktiken und -richtlinien festgelegt werden, um die
  Integration potenzieller Schwachstellen zu verhindern.
\end{enumerate}

\subsubsection{Beispiele für
Angriffsszenarien}\label{beispiele-fuxfcr-angriffsszenarien}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eine Nachrichtenorganisation nutzt ein LLM intensiv, um
  Nachrichtenartikel zu generieren. Ein böswilliger Akteur nutzt diese
  übermäßige Abhängigkeit aus, indem er das LLM mit irreführenden
  Informationen füttert und so die Verbreitung von Fehlinformationen
  verursacht.
\item
  Die KI plagiiert unbeabsichtigt Inhalte, was zu Urheberrechtsproblemen
  und einem Vertrauensverlust in die Organisation führt.
\item
  Ein Softwareentwicklungsteam verwendet ein LLM-System, um den
  Entwicklungsprozess zu beschleunigen. Die übermäßige Abhängigkeit von
  den Vorschlägen der KI führt zu Sicherheitslücken in der Anwendung
  aufgrund unsicherer Standardeinstellungen oder Empfehlungen, die nicht
  sicheren Programmierpraktiken entsprechen.
\item
  Eine Softwareentwicklungsfirma verwendet ein LLM, um Entwickelnde zu
  unterstützen. Das LLM schlägt eine nicht existierende Codebibliothek
  oder ein nicht existierendes Paket vor, und eine Person, die der KI
  vertraut, integriert unwissentlich ein schädliches Paket in die
  Software des Unternehmens. Dies unterstreicht die Bedeutung der
  Überprüfung von LLM-Vorschlägen, insbesondere wenn es sich um Code
  oder Bibliotheken von Drittanbietern handelt.
\end{enumerate}

\subsubsection{Referenzen}\label{referenzen}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://towardsdatascience.com/llm-hallucinations-ec831dcd7786}{Understanding
  LLM Hallucinations}: \textbf{Towards Data Science}
\item
  \href{https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/}{How
  Should Companies Communicate the Risks of Large Language Models to
  Users?}: \textbf{Techpolicy}
\item
  \href{https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/}{A
  news site used AI to write articles. It was a journalistic disaster}:
  \textbf{Washington Post}
\item
  \href{https://vulcan.io/blog/ai-hallucinations-package-risk}{AI
  Hallucinations: Package Risk}: \textbf{Vulcan.io}
\item
  \href{https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/}{How
  to Reduce the Hallucinations from Large Language Models}: \textbf{The
  New Stack}
\item
  \href{https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination}{Practical
  Steps to Reduce Hallucination}: \textbf{Victor Debia}
\end{enumerate}

\end{document}
