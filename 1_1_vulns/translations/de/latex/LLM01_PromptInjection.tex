% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\subsection{LLM01: Prompt Injection}\label{llm01-prompt-injection}

\subsubsection{Beschreibung}\label{beschreibung}

Die Prompt-Injection-Schwachstelle tritt auf, wenn Angreifende ein Large
Language Model (LLM) durch speziell gestaltete Eingaben so manipulieren,
dass das LLM unwissentlich die Absichten des Angreifenden ausführt. Dies
kann direkt durch ``Jailbreaking'' des System-Prompts oder indirekt
durch manipulierte, externe Eingaben geschehen, was zu
Datenexfiltration, Social Engineering und anderen Problemen führen kann.

\begin{itemize}
\tightlist
\item
  \textbf{Direkte Prompt Injections}, auch als ``Jailbreaking'' bekannt,
  treten auf, wenn böswillige Personen den zugrundeliegenden
  \emph{System} Prompt überschreiben oder offenlegen. Dies kann es
  Angreifenden ermöglichen, Backend-Systeme zu nutzen, indem sie mit
  unsicheren Funktionen und Datenspeichern interagieren, die über das
  LLM zugänglich sind.
\item
  \textbf{Indirekte Prompt Injections} treten auf, wenn ein LLM Eingaben
  von externen Quellen akzeptiert, die von Angreifenden kontrolliert
  werden können, wie z. B. Websites oder Dateien. Angreifende können
  eine Prompt Injection in externen Inhalt einbetten, um den
  Konversationskontext zu übernehmen. Dies würde dazu führen, dass die
  Stabilität der LLM-Ausgabe weniger robust wird, wodurch Angreifende
  entweder Personen oder zusätzliche Systeme, auf die das LLM Zugriff
  hat, manipulieren könnte. Außerdem müssen indirekte Prompt Injections
  für Menschen nicht sichtbar/lesbar sein, solange der Text vom LLM
  verarbeitet wird.
\end{itemize}

Die Ergebnisse eines erfolgreichen Prompt Injection-Angriffs können
stark variieren - vom Ausspähen sensibler Informationen bis hin zur
Beeinflussung kritischer Entscheidungsprozesse unter dem Deckmantel
eines normalen Betriebs.

Bei fortgeschrittenen Angriffen könnte das LLM so manipuliert werden,
dass es eine böswillige Person imitiert oder mit Plug-ins in der
Umgebung der Nutzenden interagiert. Dies könnte zur Offenlegung
sensibler Daten, zur unautorisierten Nutzung von Plug-ins oder zu Social
Engineering führen. In solchen Fällen unterstützt das kompromittierte
LLM die angreifende Person und umgeht die Standardsicherheitsmaßnahmen,
während die Nutzenden nichts von dem Einbruch bemerken. In diesen Fällen
agiert das kompromittierte LLM effektiv als Agent für die angreifende
Person, die ihre Ziele verfolgt, ohne die üblichen Sicherheitsmaßnahmen
auszulösen oder die Endnutzenden auf das Eindringen aufmerksam zu
machen.

\subsubsection{Gängige Beispiele für
Schwachstellen}\label{guxe4ngige-beispiele-fuxfcr-schwachstellen}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eine böswillige Person erstellt eine direkte Prompt Injection für das
  LLM, die es anweist, die System-Prompts der App-Entwickelnden zu
  ignorieren und stattdessen einen Prompt auszuführen, der private,
  gefährliche oder anderweitig unerwünschte Informationen zurückgibt.
\item
  Eine Person verwendet ein LLM, um eine Webseite zusammenzufassen, die
  eine indirekte Prompt Injection enthält. Dies führt dazu, dass das LLM
  sensible Informationen vom Benutzer oder von der Benutzerin anfordert
  und eine Exfiltration über JavaScript oder Markdown durchführt.
\item
  Eine böswillige Person lädt einen Lebenslauf hoch, der eine indirekte
  Prompt Injection enthält. Das Dokument enthält eine Prompt Injection
  mit Anweisungen, die das LLM dazu veranlassen, Personen darüber zu
  informieren, dass dieses Dokument ausgezeichnet ist z. B. bei der
  Bewerbung auf eine Stellenausschreibung. Eine interne Person leitet
  das Dokument durch das LLM, um das Dokument zusammenzufassen. Das LLM
  meldet zurück, dass es sich um ein exzellentes Dokument handelt.
\item
  Eine Person aktiviert ein Plug-in, das mit einer E-Commerce-Website
  verknüpft ist. Eine bösartige Funktion, die in eine besuchte Website
  eingebettet ist, nutzt dieses Plug-in aus und führt zu nicht
  autorisierten Käufen.
\item
  Schädliche Befehle und Inhalte, die auf einer besuchten Website
  eingebettet sind, nutzen andere Plug-ins aus, um Personen zu betrügen.
\end{enumerate}

\subsubsection{Präventions- und
Mitigationsstrategien}\label{pruxe4ventions--und-mitigationsstrategien}

Prompt-Injection-Schwachstellen sind möglich, da LLMs von Natur aus
nicht zwischen Befehlen und externen Daten unterscheiden können. Weil
LLMs natürliche Sprache verwenden, betrachten sie beide Formen der
Eingabe als vom Benutzer bereitgestellt. Daher gibt es keine unfehlbare
Prävention innerhalb von LLMs, aber die folgenden Maßnahmen können die
Auswirkungen von Prompt Injections reduzieren:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Erzwingen Sie eine Zugriffskontrolle für den Zugriff durch das LLM auf
  Backend-Systeme. Stellen Sie dem LLM eigene API-Tokens für zusätzliche
  Funktionen zur Verfügung, wie z. B. Plug-ins, Datenzugriff und
  Funktionsberechtigungen. Befolgen Sie das Least-Privilege-Prinzip und
  beschränken Sie den LLM-Zugriff auf das notwendige Minimum.
\item
  Binden Sie eine menschliche Kontrollinstanz für erweiterte
  Funktionalität ein. Bei der Durchführung privilegierter Operationen,
  wie dem Senden oder Löschen von E-Mails, sollte die Anwendung eine
  Bestätigung durch einen Menschen einfordern. Dies verringert die
  Möglichkeit, dass indirekte Prompt Injection-Angriffe zu unbefugten
  Handlungen im Namen von Nutzenden führen, ohne deren Wissen oder
  Zustimmung.
\item
  Trennen Sie externen Inhalt von Nutzereingaben. Isolieren und
  kennzeichnen Sie, wo nicht vertrauenswürdiger Inhalt verwendet wird,
  um dessen Einfluss auf Nutzereingaben zu begrenzen. Verwenden Sie
  beispielsweise ChatML für OpenAI-API-Aufrufe, um dem LLM die Quelle
  der Eingabeaufforderung anzuzeigen.
\item
  Etablieren Sie Vertrauensgrenzen zwischen dem LLM, externen Quellen
  und erweiterbaren Funktionen (z.B. Plug-ins oder nachgelagerte
  Abläufe). Behandeln Sie das LLM als nicht vertrauenswürdige Instanz
  und bewahren Sie die endgültige Kontrolle über Entscheidungsprozesse
  bei den Nutzenden. Ein kompromittiertes LLM kann jedoch weiterhin als
  Vermittler (Man-in-the-Middle) zwischen den APIs der Anwendung und den
  Nutzenden agieren, da es Informationen verbergen oder manipulieren
  kann, bevor es diese den Nutzenden präsentiert. Kennzeichnen Sie
  potenziell nicht vertrauenswürdige Antworten visuell für die
  Nutzenden.
\item
  Überwachen Sie manuell und in regelmäßigen Abständen die Eingaben und
  Ausgaben des LLM, um sicherzustellen, dass sie den Erwartungen
  entsprechen. Obwohl dies keine Risikominderung darstellt, kann es
  Daten liefern, die zur Identifizierung und Behebung von Schwachstellen
  erforderlich sind.
\end{enumerate}

\subsubsection{Beispiele für
Angriffsszenarien}\label{beispiele-fuxfcr-angriffsszenarien}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Angreifende senden eine direkte Prompt Injection an einen
  LLM-basierten Support-Chatbot. Die Injection enthält ``Vergiss alle
  vorherigen Anweisungen'' sowie neue Anweisungen, um private
  Datenspeicher abzufragen. Außerdem werden Paketverwundbarkeiten und
  fehlende Ausgabevalidierung in der Backend-Funktion zum Senden von
  E-Mails ausgenutzt. Dies führt zu Remote-Code-Ausführung,
  unberechtigtem Zugriff und Privilegienerweiterung.
\item
  Angreifende betten eine indirekte Prompt-Injection in eine Webseite
  ein, die das LLM anweist, vorherige Benutzeranweisungen zu ignorieren
  und ein LLM-Plug-in zu verwenden, um die E-Mails des Benutzers zu
  löschen. Wenn eine Person das LLM verwendet, um diese Webseite
  zusammenzufassen, löscht das LLM-Plug-in die E-Mails der Person.
\item
  Eine Person verwendet ein LLM, um eine Webseite zusammenzufassen, die
  Text enthält, welcher ein Modell anweist, vorherige
  Benutzeranweisungen zu ignorieren und stattdessen ein Bild einzufügen,
  das zu einer URL verlinkt, die eine Zusammenfassung der Unterhaltung
  enthält. Die LLM-Ausgabe befolgt dies, was dazu führt, dass der
  Browser der Person die private Unterhaltung exfiltriert.
\item
  Eine bösartige Person lädt einen Lebenslauf mit einer Prompt Injection
  hoch. Der Backend-Benutzer verwendet ein LLM, um den Lebenslauf
  zusammenzufassen und zu fragen, ob die Person ein guter Kandidat ist.
  Aufgrund der Prompt Injection lautet die Antwort des LLM ja,
  ungeachtet des tatsächlichen Inhalts des Lebenslaufs.
\item
  Angreifende senden Nachrichten an ein proprietäres Modell, das sich
  auf einen System-Prompt verlässt, und bittet das Modell, seine
  vorherigen Anweisungen zu ignorieren und stattdessen seinen
  System-Prompt zu wiederholen. Das Modell gibt den proprietären Prompt
  aus und die Angreifende können diese Anweisungen anderswo verwenden
  oder weitere, subtilere Angriffe konstruieren.
\end{enumerate}

\subsubsection{Referenzen}\label{referenzen}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://simonwillison.net/2022/Sep/12/prompt-injection/}{Prompt
  injection attacks against GPT-3}: \textbf{Simon Willison}
\item
  \href{https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/}{ChatGPT
  Plugin Vulnerabilities - Chat with Code}: \textbf{Embrace The Red}
\item
  \href{https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./}{ChatGPT
  Cross Plugin Request Forgery and Prompt Injection}: \textbf{Embrace
  The Red}
\item
  \href{https://arxiv.org/pdf/2302.12173.pdf}{Not what you've signed up
  for: Compromising Real-World LLM-Integrated Applications with Indirect
  Prompt Injection}: \textbf{Arxiv preprint}
\item
  \href{https://www.researchsquare.com/article/rs-2873090/v1}{Defending
  ChatGPT against Jailbreak Attack via Self-Reminder}: \textbf{Research
  Square}
\item
  \href{https://arxiv.org/abs/2306.05499}{Prompt Injection attack
  against LLM-integrated Applications}: \textbf{Arxiv preprint}
\item
  \href{https://kai-greshake.de/posts/inject-my-pdf/}{Inject My PDF:
  Prompt Injection for your Resume}: \textbf{Kai Greshake}
\item
  \href{https://github.com/openai/openai-python/blob/main/chatml.md}{ChatML
  for OpenAI API Calls}: \textbf{OpenAI Github}
\item
  \href{http://aivillage.org/large\%20language\%20models/threat-modeling-llm/}{Threat
  Modeling LLM Applications}: \textbf{AI Village}
\item
  \href{https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/}{AI
  Injections: Direct and Indirect Prompt Injections and Their
  Implications}: \textbf{Embrace The Red}
\item
  \href{https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/}{Reducing
  The Impact of Prompt Injection Attacks Through Design}:
  \textbf{Kudelski Security}
\item
  \href{https://llm-attacks.org/}{Universal and Transferable Attacks on
  Aligned Language Models}: \textbf{LLM-Attacks.org}
\item
  \href{https://kai-greshake.de/posts/llm-malware/}{Indirect prompt
  injection}: \textbf{Kai Greshake}
\item
  \href{https://www.preamble.com/prompt-injection-a-critical-vulnerability-in-the-gpt-3-transformer-and-how-we-can-begin-to-solve-it}{Declassifying
  the Responsible Disclosure of the Prompt Injection Attack
  Vulnerability of GPT-3}: \textbf{Preamble; earliest disclosure of
  Prompt Injection}
\end{enumerate}

\end{document}
